{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac923140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "WGAN-GP增强与XGBoost分类的严谨实验框架\n",
    "=================================================\n",
    "本脚本实现了一个完整的、符合高水平学术研究标准的机器学习流程，用于心血管疾病的二元分类预测。\n",
    "核心方法学包括：\n",
    "1.  **严格的数据隔离**: 在流程开始时就将数据分为独立的“开发集”和“最终测试集”，确保最终评估的无偏性。\n",
    "2.  **动态嵌套式数据增强**: 在K-折交叉验证的每一折内部，动态地使用当前折的训练数据来训练WGAN-GP并生成增强样本，从根本上杜绝了数据泄露。\n",
    "3.  **混合数据类型处理**: WGAN-GP流程内置了对连续特征（标准化）和分类特征（独热编码）的自动化处理与逆转换。\n",
    "4.  **正确的模型与评估**: 全程使用XGBoost分类器（XGBClassifier）以及与之匹配的分类性能指标（如Accuracy, F1 Score, AUC）进行模型训练、调优和评估。\n",
    "\"\"\"\n",
    "\n",
    "# --- 基础库导入 ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, ConfusionMatrixDisplay,\n",
    "                             make_scorer)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 0. 全局配置 ---\n",
    "\n",
    "# --- 路径配置 ---\n",
    "# 建议将此脚本放在一个名为 \"scripts\" 的文件夹中，\n",
    "# \"data\" 和 \"output\" 文件夹与 \"scripts\" 文件夹在同一级。\n",
    "# 示例目录结构:\n",
    "# /your_project_folder/\n",
    "#  ├─ data/\n",
    "#  │  ├─ development_set.xlsx\n",
    "#  │  └─ final_test_set.xlsx\n",
    "#  ├─ scripts/\n",
    "#  │  └─ this_script.py\n",
    "#  └─ output/\n",
    "#     ├─ augmented_outputs/\n",
    "#     └─ trained_models/\n",
    "\n",
    "CURRENT_DIR = Path.cwd()\n",
    "# 假设脚本在 'scripts' 文件夹内，项目根目录是其父目录\n",
    "PROJECT_ROOT = CURRENT_DIR.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output\"\n",
    "\n",
    "# 数据文件路径\n",
    "DEV_SET_FILE = DATA_DIR / \"development_set.xlsx\"\n",
    "TEST_SET_FILE = DATA_DIR / \"final_test_set.xlsx\"\n",
    "# 中间生成文件和最终模型的保存路径\n",
    "AUGMENTED_DATA_OUTPUT_FOLDER = OUTPUT_DIR / \"augmented_outputs\"\n",
    "MODEL_OUTPUT_PATH = OUTPUT_DIR / \"trained_models\"\n",
    "OUTPUT_PLOT_PATH = OUTPUT_DIR # 图表导出路径\n",
    "\n",
    "# 自动创建不存在的输出文件夹\n",
    "os.makedirs(AUGMENTED_DATA_OUTPUT_FOLDER, exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(OUTPUT_PLOT_PATH, exist_ok=True)\n",
    "\n",
    "# --- 实验参数 ---\n",
    "TARGET_COLUMN = 'target'  # 目标变量（标签）的列名\n",
    "RANDOM_STATE = 42         # 随机种子，确保实验可复现\n",
    "N_SPLITS_KFOLD = 5        # K-Fold交叉验证的折数\n",
    "\n",
    "# --- WGAN-GP 预设参数 ---\n",
    "DEFAULT_WGAN_PARAMS = {\n",
    "    'latent_dim': 100,      # 随机噪声向量的维度\n",
    "    'lambda_gp': 10,        # 梯度惩罚的系数\n",
    "    'n_critic': 5,          # 每次生成器更新前，判别器的更新次数\n",
    "    'lr': 0.00005,          # 学习率\n",
    "    'batch_size': 32,       # 批处理大小\n",
    "    'epochs': 500           # 默认训练轮数 (在CV中会使用)\n",
    "}\n",
    "\n",
    "# --- XGBoost RandomizedSearchCV 超参数搜索空间 ---\n",
    "XGB_PARAM_GRID = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.01, 0.1],\n",
    "    'reg_lambda': [0.5, 1, 1.5],\n",
    "    'scale_pos_weight': [1, 5, 10, 20] # 处理类别不平衡的关键参数\n",
    "}\n",
    "N_ITER_RANDOMIZED_SEARCH = 30 # RandomizedSearchCV的迭代次数\n",
    "\n",
    "# --- 设备配置 ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"将使用设备: {device}\")\n",
    "\n",
    "# --- 数据特征定义 ---\n",
    "# 连续型特征列名\n",
    "CONTINUOUS_COLS = ['age', 'restingBP', 'serumcholestrol', 'maxheartrate', 'oldpeak']\n",
    "# 分类型特征列名 (包含目标变量，因为GAN需要学习其联合分布)\n",
    "CATEGORICAL_COLS = ['gender', 'fastingbloodsugar', 'chestpain',\n",
    "                    'restingrelectro', 'exerciseangia', 'slope',\n",
    "                    'noofmajorvessels', 'target']\n",
    "\n",
    "# 分类型特征的合法取值范围 (用于生成数据后的后处理)\n",
    "CATEGORY_MAPPINGS = {\n",
    "    'gender':           [0, 1],\n",
    "    'fastingbloodsugar':[0, 1],\n",
    "    'chestpain':        [0, 1, 2, 3],\n",
    "    'restingrelectro':  [0, 1, 2],\n",
    "    'exerciseangia':    [0, 1],\n",
    "    'slope':            [1, 2, 3], # 已根据数据描述修正，包含0\n",
    "    'noofmajorvessels': [0, 1, 2, 3],\n",
    "    'target':           [0, 1]\n",
    "}\n",
    "\n",
    "# 连续型特征的合理值域 (用于生成数据后的裁剪)\n",
    "CONTINUOUS_BOUNDS = {\n",
    "    'age':              (20, 80),\n",
    "    'restingBP':        (94, 200),\n",
    "    'serumcholestrol':  (126, 564),\n",
    "    'maxheartrate':     (71, 202),\n",
    "    'oldpeak':          (0, 6.2)\n",
    "}\n",
    "\n",
    "# --- 1. WGAN-GP 模型定义与核心功能函数 ---\n",
    "\n",
    "# --- WGAN-GP PyTorch网络结构 ---\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"生成器网络\"\"\"\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128), nn.ReLU(True),\n",
    "            nn.Linear(128, 256),      nn.ReLU(True),\n",
    "            nn.Linear(256, 512),      nn.ReLU(True),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"判别器/评论家网络\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(512, 256),       nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(256, 128),       nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def gradient_penalty(critic, real, fake, device):\n",
    "    \"\"\"计算梯度惩罚项\"\"\"\n",
    "    alpha = torch.rand(real.size(0), 1, device=device)\n",
    "    interpolates = (alpha * real + (1 - alpha) * fake).requires_grad_(True)\n",
    "    d_interpolates = critic(interpolates)\n",
    "    grad_out = torch.ones_like(d_interpolates, device=device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates, inputs=interpolates,\n",
    "        grad_outputs=grad_out,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "# --- WGAN-GP 训练与生成主函数 ---\n",
    "def train_and_generate_wgangp(input_original_df,\n",
    "                              wgan_hyperparams,\n",
    "                              num_samples_to_generate,\n",
    "                              current_device,\n",
    "                              fold_num_for_logging=None,\n",
    "                              output_augmented_data_path=None):\n",
    "    \"\"\"\n",
    "    一个完整的函数，用于训练WGAN-GP并生成指定数量的增强样本。\n",
    "    该函数封装了所有预处理、训练、生成和后处理步骤。\n",
    "    \"\"\"\n",
    "    log_prefix = f\"[WGAN-GP\"\n",
    "    if fold_num_for_logging:\n",
    "        log_prefix += f\" {fold_num_for_logging}\"\n",
    "    log_prefix += \"]\"\n",
    "    print(f\"\\n{log_prefix}] 开始数据增强，输入形状: {input_original_df.shape}\")\n",
    "\n",
    "    # 1. 预处理：拆分、标准化、独热编码\n",
    "    cont_df = input_original_df[CONTINUOUS_COLS]\n",
    "    cat_df  = input_original_df[CATEGORICAL_COLS]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    cont_std = scaler.fit_transform(cont_df)\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False, dtype=np.float32, handle_unknown='ignore')\n",
    "    cat_oh  = encoder.fit_transform(cat_df)\n",
    "\n",
    "    # 2. 准备PyTorch DataLoader\n",
    "    X_processed = np.hstack([cont_std, cat_oh]).astype(np.float32)\n",
    "    data_tensor = torch.tensor(X_processed)\n",
    "    bs = min(wgan_hyperparams['batch_size'], len(data_tensor))\n",
    "    dataloader = DataLoader(TensorDataset(data_tensor),\n",
    "                            batch_size=bs,\n",
    "                            shuffle=True,\n",
    "                            drop_last=len(data_tensor) >= bs * 2)\n",
    "\n",
    "    # 3. 初始化网络与优化器\n",
    "    latent_dim = wgan_hyperparams['latent_dim']\n",
    "    netG = Generator(latent_dim, X_processed.shape[1]).to(current_device)\n",
    "    netD = Critic(X_processed.shape[1]).to(current_device)\n",
    "    optG = optim.RMSprop(netG.parameters(), lr=wgan_hyperparams['lr'])\n",
    "    optD = optim.RMSprop(netD.parameters(), lr=wgan_hyperparams['lr'])\n",
    "\n",
    "    # 4. 训练循环\n",
    "    print(f\"{log_prefix}] 开始WGAN-GP训练 ({wgan_hyperparams['epochs']} 轮)...\")\n",
    "    epochs = wgan_hyperparams['epochs']\n",
    "    for epoch in range(epochs):\n",
    "        for real, in dataloader:\n",
    "            real = real.to(current_device)\n",
    "            # 训练判别器\n",
    "            for _ in range(wgan_hyperparams['n_critic']):\n",
    "                z = torch.randn(real.size(0), latent_dim, device=current_device)\n",
    "                fake = netG(z)\n",
    "                lossD = (torch.mean(netD(fake)) -\n",
    "                         torch.mean(netD(real))) + wgan_hyperparams['lambda_gp'] * gradient_penalty(netD, real, fake, current_device)\n",
    "                optD.zero_grad(); lossD.backward(); optD.step()\n",
    "            # 训练生成器\n",
    "            z = torch.randn(real.size(0), latent_dim, device=current_device)\n",
    "            lossG = -torch.mean(netD(netG(z)))\n",
    "            optG.zero_grad(); lossG.backward(); optG.step()\n",
    "        if (epoch + 1) % max(1, epochs // 10) == 0:\n",
    "            print(f\"{log_prefix}] [Epoch {epoch+1}/{epochs}] Critic Loss: {lossD.item():.4f}, Gen Loss: {lossG.item():.4f}\")\n",
    "\n",
    "    # 5. 生成新样本\n",
    "    print(f\"{log_prefix}] 训练完成，正在生成 {num_samples_to_generate} 个样本...\")\n",
    "    netG.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples_to_generate, latent_dim, device=current_device)\n",
    "        gen_processed = netG(z).cpu().numpy()\n",
    "\n",
    "    # 6. 后处理：反变换与数据还原\n",
    "    num_cont_cols = len(CONTINUOUS_COLS)\n",
    "    gen_cont_std = gen_processed[:, :num_cont_cols]\n",
    "    gen_cat_oh   = gen_processed[:, num_cont_cols:]\n",
    "\n",
    "    # 6a. 连续列反标准化 + 裁剪\n",
    "    gen_cont = scaler.inverse_transform(gen_cont_std)\n",
    "    for i, col in enumerate(CONTINUOUS_COLS):\n",
    "        lo, hi = CONTINUOUS_BOUNDS[col]\n",
    "        gen_cont[:, i] = np.clip(gen_cont[:, i], lo, hi)\n",
    "\n",
    "    # 6b. 分类列反独热编码 + 映射到合法类别 (修正后的鲁棒版本)\n",
    "    gen_cat_df = pd.DataFrame(columns=CATEGORICAL_COLS)\n",
    "    current_col_idx = 0\n",
    "    # encoder.categories_ 会按顺序存储每个分类特征的类别\n",
    "    for i, col in enumerate(CATEGORICAL_COLS):\n",
    "        # 获取当前特征的独热编码有多少列\n",
    "        num_categories = len(encoder.categories_[i])\n",
    "        # 切片出对应的独热编码部分\n",
    "        col_slice = gen_cat_oh[:, current_col_idx : current_col_idx + num_categories]\n",
    "        # 找到每行最大值的索引，这个索引就是类别在 categories_ 中的位置\n",
    "        # argmax确保即使有多个最大值，也只返回第一个，避免歧义\n",
    "        cat_indices = np.argmax(col_slice, axis=1)\n",
    "        # 从 encoder 中恢复原始类别标签\n",
    "        original_labels = encoder.categories_[i][cat_indices]\n",
    "        gen_cat_df[col] = original_labels\n",
    "        # 更新下一个特征的起始索引\n",
    "        current_col_idx += num_categories\n",
    "\n",
    "    # 将所有列转换为数值类型，为下一步映射做准备\n",
    "    for col in CATEGORICAL_COLS:\n",
    "        gen_cat_df[col] = pd.to_numeric(gen_cat_df[col], errors='coerce')\n",
    "\n",
    "    # 对可能不是整数的生成值，映射到最近的合法类别\n",
    "    for col in CATEGORICAL_COLS:\n",
    "        gen_cat_df[col] = gen_cat_df[col].apply(\n",
    "            lambda x: min(CATEGORY_MAPPINGS[col], key=lambda v: abs(v - x)) if pd.notna(x) else x\n",
    "        )\n",
    "    \n",
    "    # 7. 合并为最终的DataFrame\n",
    "    augmented_df = pd.DataFrame(gen_cont, columns=CONTINUOUS_COLS)\n",
    "    augmented_df[CATEGORICAL_COLS] = gen_cat_df\n",
    "\n",
    "    # 8. (可选) 保存到文件\n",
    "    if output_augmented_data_path:\n",
    "        augmented_df.to_excel(output_augmented_data_path, index=False)\n",
    "        print(f\"{log_prefix}] 增强数据已保存到: {output_augmented_data_path}\")\n",
    "\n",
    "    return augmented_df\n",
    "\n",
    "# --- 2. 主流程开始 ---\n",
    "try:\n",
    "    development_df_original = pd.read_excel(DEV_SET_FILE)\n",
    "    final_test_df_original = pd.read_excel(TEST_SET_FILE)\n",
    "    print(f\"成功加载数据: 开发集形状 {development_df_original.shape}, 最终测试集形状 {final_test_df_original.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"错误: 数据文件未找到，请检查路径。 {e}\")\n",
    "    exit()\n",
    "\n",
    "# 将开发集和最终测试集划分为特征(X)和标签(y)\n",
    "X_dev_original = development_df_original.drop(columns=[TARGET_COLUMN])\n",
    "y_dev_original = development_df_original[TARGET_COLUMN]\n",
    "X_final_test = final_test_df_original.drop(columns=[TARGET_COLUMN])\n",
    "y_final_test = final_test_df_original[TARGET_COLUMN]\n",
    "\n",
    "# --- 步骤 3.1: 超参数调优 (HPO) ---\n",
    "print(\"\\n--- [步骤 3.1] XGBoost 超参数调优 ---\")\n",
    "print(\"为HPO生成开发集的增强版本...\")\n",
    "num_augmented_samples_for_hpo = len(development_df_original) * 1 # 生成与原始数据等量的样本\n",
    "hpo_wgan_params = DEFAULT_WGAN_PARAMS.copy()\n",
    "hpo_wgan_params['epochs'] = 1000 # HPO时可以适当减少轮数以节省时间\n",
    "\n",
    "augmented_dev_for_hpo_df = train_and_generate_wgangp(\n",
    "    input_original_df=development_df_original.copy(),\n",
    "    wgan_hyperparams=hpo_wgan_params,\n",
    "    num_samples_to_generate=num_augmented_samples_for_hpo,\n",
    "    current_device=device,\n",
    "    fold_num_for_logging=\"HPO_Dev_Set\"\n",
    ")\n",
    "\n",
    "# 准备用于HPO的数据集 (原始 + 增强)\n",
    "X_augmented_dev_for_hpo = augmented_dev_for_hpo_df.drop(columns=[TARGET_COLUMN])\n",
    "y_augmented_dev_for_hpo = augmented_dev_for_hpo_df[TARGET_COLUMN]\n",
    "X_combined_dev_for_hpo = pd.concat([X_dev_original, X_augmented_dev_for_hpo], ignore_index=True)\n",
    "y_combined_dev_for_hpo = pd.concat([y_dev_original, y_augmented_dev_for_hpo], ignore_index=True)\n",
    "\n",
    "# 初始化XGBoost分类器和RandomizedSearchCV\n",
    "xgb_classifier_for_hpo = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss',\n",
    "                                           random_state=RANDOM_STATE, use_label_encoder=False,\n",
    "                                           tree_method='gpu_hist' if device.type == 'cuda' else 'auto')\n",
    "\n",
    "random_search_hpo = RandomizedSearchCV(\n",
    "    estimator=xgb_classifier_for_hpo, param_distributions=XGB_PARAM_GRID,\n",
    "    n_iter=N_ITER_RANDOMIZED_SEARCH, cv=N_SPLITS_KFOLD,\n",
    "    scoring='roc_auc', # 使用AUC作为评估指标，对不平衡数据更稳健\n",
    "    verbose=1, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "print(f\"开始在 {X_combined_dev_for_hpo.shape[0]} 个样本上进行XGBoost超参数搜索...\")\n",
    "random_search_hpo.fit(X_combined_dev_for_hpo, y_combined_dev_for_hpo)\n",
    "best_overall_xgboost_params = random_search_hpo.best_params_\n",
    "print(f\"\\n找到的最佳XGBoost超参数: {best_overall_xgboost_params}\")\n",
    "print(f\"最佳HPO ROC AUC Score: {random_search_hpo.best_score_:.4f}\")\n",
    "\n",
    "# --- 步骤 3.2: K-折交叉验证与动态增强 ---\n",
    "print(f\"\\n--- [步骤 3.2] 在开发集上进行 {N_SPLITS_KFOLD}-折交叉验证 (动态WGAN-GP增强) ---\")\n",
    "kf = KFold(n_splits=N_SPLITS_KFOLD, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# 用于存储每折结果的列表\n",
    "kfold_cv_val_metrics_list = []\n",
    "kfold_cv_train_metrics_list = []\n",
    "cv_train_loss_curves = []\n",
    "cv_val_loss_curves = []\n",
    "\n",
    "cv_wgan_params = DEFAULT_WGAN_PARAMS.copy()\n",
    "augmentation_factor_cv = 1 # 在CV中，为训练集生成等量的增强数据\n",
    "\n",
    "for fold_idx, (train_indices, val_indices) in enumerate(kf.split(development_df_original)):\n",
    "    print(f\"\\n--- K-Fold: 第 {fold_idx + 1}/{N_SPLITS_KFOLD} 折 ---\")\n",
    "    cv_train_original_fold_df = development_df_original.iloc[train_indices]\n",
    "    cv_val_original_fold_df = development_df_original.iloc[val_indices]\n",
    "\n",
    "    # 在当前训练折上动态生成增强数据\n",
    "    cv_augmented_fold_df = train_and_generate_wgangp(\n",
    "        input_original_df=cv_train_original_fold_df.copy(),\n",
    "        wgan_hyperparams=cv_wgan_params,\n",
    "        num_samples_to_generate=len(cv_train_original_fold_df) * augmentation_factor_cv,\n",
    "        current_device=device,\n",
    "        fold_num_for_logging=f\"Fold_{fold_idx + 1}\"\n",
    "    )\n",
    "\n",
    "    # 准备当前折的训练集(原始+增强)和验证集(仅原始)\n",
    "    X_cv_train_original_fold = cv_train_original_fold_df.drop(columns=[TARGET_COLUMN])\n",
    "    y_cv_train_original_fold = cv_train_original_fold_df[TARGET_COLUMN]\n",
    "    X_cv_augmented_fold = cv_augmented_fold_df.drop(columns=[TARGET_COLUMN])\n",
    "    y_cv_augmented_fold = cv_augmented_fold_df[TARGET_COLUMN]\n",
    "    X_cv_train_combined_fold = pd.concat([X_cv_train_original_fold, X_cv_augmented_fold], ignore_index=True)\n",
    "    y_cv_train_combined_fold = pd.concat([y_cv_train_original_fold, y_cv_augmented_fold], ignore_index=True)\n",
    "    \n",
    "    X_cv_val_fold = cv_val_original_fold_df.drop(columns=[TARGET_COLUMN])\n",
    "    y_cv_val_fold = cv_val_original_fold_df[TARGET_COLUMN]\n",
    "\n",
    "    # 使用找到的最佳参数训练模型\n",
    "    model_fold = xgb.XGBClassifier(\n",
    "        **best_overall_xgboost_params, objective='binary:logistic', eval_metric='logloss',\n",
    "        random_state=RANDOM_STATE, use_label_encoder=False,\n",
    "        tree_method='gpu_hist' if device.type == 'cuda' else 'auto'\n",
    "    )\n",
    "    \n",
    "    eval_set_fold = [(X_cv_train_combined_fold, y_cv_train_combined_fold), (X_cv_val_fold, y_cv_val_fold)]\n",
    "    model_fold.fit(X_cv_train_combined_fold, y_cv_train_combined_fold,\n",
    "                   eval_set=eval_set_fold, early_stopping_rounds=10, verbose=False)\n",
    "\n",
    "    # 记录损失曲线\n",
    "    fold_eval_results = model_fold.evals_result()\n",
    "    cv_train_loss_curves.append(fold_eval_results['validation_0']['logloss'])\n",
    "    cv_val_loss_curves.append(fold_eval_results['validation_1']['logloss'])\n",
    "\n",
    "    # 在验证集上评估\n",
    "    y_pred_val_proba = model_fold.predict_proba(X_cv_val_fold)[:, 1]\n",
    "    y_pred_val = (y_pred_val_proba > 0.5).astype(int)\n",
    "    kfold_cv_val_metrics_list.append({\n",
    "        'Accuracy': accuracy_score(y_cv_val_fold, y_pred_val),\n",
    "        'F1 Score': f1_score(y_cv_val_fold, y_pred_val),\n",
    "        'AUC': roc_auc_score(y_cv_val_fold, y_pred_val_proba)\n",
    "    })\n",
    "\n",
    "    # 在训练集上评估 (用于监控过拟合)\n",
    "    y_pred_train_proba = model_fold.predict_proba(X_cv_train_combined_fold)[:, 1]\n",
    "    y_pred_train = (y_pred_train_proba > 0.5).astype(int)\n",
    "    kfold_cv_train_metrics_list.append({\n",
    "        'Accuracy': accuracy_score(y_cv_train_combined_fold, y_pred_train),\n",
    "        'F1 Score': f1_score(y_cv_train_combined_fold, y_pred_train),\n",
    "        'AUC': roc_auc_score(y_cv_train_combined_fold, y_pred_train_proba)\n",
    "    })\n",
    "    print(f\"Fold {fold_idx + 1} - Val AUC: {kfold_cv_val_metrics_list[-1]['AUC']:.4f} | Train AUC: {kfold_cv_train_metrics_list[-1]['AUC']:.4f}\")\n",
    "\n",
    "# --- 步骤 3.3: 交叉验证结果分析与可视化 ---\n",
    "# 计算平均性能\n",
    "avg_kfold_cv_val_metrics_df = pd.DataFrame(kfold_cv_val_metrics_list)\n",
    "avg_kfold_cv_train_metrics_df = pd.DataFrame(kfold_cv_train_metrics_list)\n",
    "\n",
    "print(\"\\n--- K-折交叉验证平均性能 (WGAN-GP动态增强) ---\")\n",
    "print(\"--- 平均验证集性能 ---\")\n",
    "print(avg_kfold_cv_val_metrics_df.mean())\n",
    "print(\"\\n--- 平均训练集性能 ---\")\n",
    "print(avg_kfold_cv_train_metrics_df.mean())\n",
    "\n",
    "# 可视化1: 平均性能指标对比条形图\n",
    "avg_val_metrics = avg_kfold_cv_val_metrics_df.mean()\n",
    "avg_train_metrics = avg_kfold_cv_train_metrics_df.mean()\n",
    "metrics_to_plot = ['Accuracy', 'F1 Score', 'AUC']\n",
    "x_axis = np.arange(len(metrics_to_plot))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x_axis - 0.2, avg_train_metrics[metrics_to_plot], width=0.4, label='CV Train Avg.', align='center')\n",
    "plt.bar(x_axis + 0.2, avg_val_metrics[metrics_to_plot], width=0.4, label='CV Validation Avg.', align='center')\n",
    "plt.xticks(x_axis, metrics_to_plot)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Average K-Fold CV Train vs. Validation Metrics (WGAN-GP Augmented)')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.ylim(0.8, 1.0) # 根据实际情况调整Y轴范围，以更好地显示差异\n",
    "plt.savefig(OUTPUT_PLOT_PATH / \"kfold_avg_eval_metrics_wgangp.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 可视化2: 每折的训练/验证损失曲线\n",
    "plt.figure(figsize=(12, 7))\n",
    "for i in range(N_SPLITS_KFOLD):\n",
    "    plt.plot(cv_train_loss_curves[i], label=f'Train Fold {i+1}', linestyle='--')\n",
    "    plt.plot(cv_val_loss_curves[i], label=f'Validation Fold {i+1}', linestyle='-')\n",
    "plt.xlabel('Boosting Round')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Training and Validation Log Loss per Fold (WGAN-GP Augmented)')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.savefig(OUTPUT_PLOT_PATH / \"kfold_logloss_per_fold_wgangp.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- 步骤 4: 训练最终模型 ---\n",
    "print(\"\\n--- [步骤 4] 训练最终模型 ---\")\n",
    "print(\"为最终模型生成开发集的完整增强版本...\")\n",
    "final_wgan_params = DEFAULT_WGAN_PARAMS.copy()\n",
    "final_wgan_params['epochs'] = 2000 # 最终模型使用更多的训练轮数\n",
    "num_augmented_samples_final = len(development_df_original) * 2 # 生成2倍的增强数据\n",
    "\n",
    "final_augmented_dev_df = train_and_generate_wgangp(\n",
    "    input_original_df=development_df_original.copy(),\n",
    "    wgan_hyperparams=final_wgan_params,\n",
    "    num_samples_to_generate=num_augmented_samples_final,\n",
    "    current_device=device,\n",
    "    fold_num_for_logging=\"Final_Model_Augmentation\",\n",
    "    output_augmented_data_path=AUGMENTED_DATA_OUTPUT_FOLDER / \"augmented_for_final_model.xlsx\"\n",
    ")\n",
    "\n",
    "# 准备最终训练数据\n",
    "X_final_augmented_dev = final_augmented_dev_df.drop(columns=[TARGET_COLUMN])\n",
    "y_final_augmented_dev = final_augmented_dev_df[TARGET_COLUMN]\n",
    "X_train_final_model = pd.concat([X_dev_original, X_final_augmented_dev], ignore_index=True)\n",
    "y_train_final_model = pd.concat([y_dev_original, y_final_augmented_dev], ignore_index=True)\n",
    "print(f\"用于训练最终模型的总数据形状: {X_train_final_model.shape}\")\n",
    "\n",
    "# 初始化并训练最终模型\n",
    "final_model = xgb.XGBClassifier(\n",
    "    **best_overall_xgboost_params, objective='binary:logistic', eval_metric='logloss',\n",
    "    random_state=RANDOM_STATE, use_label_encoder=False,\n",
    "    tree_method='gpu_hist' if device.type == 'cuda' else 'auto'\n",
    ")\n",
    "print(\"开始训练最终模型...\")\n",
    "final_model.fit(X_train_final_model, y_train_final_model)\n",
    "print(\"最终模型训练完成。\")\n",
    "\n",
    "# 保存最终模型\n",
    "final_model_path = MODEL_OUTPUT_PATH / \"final_xgboost_wgangp_model.joblib\"\n",
    "joblib.dump(final_model, final_model_path)\n",
    "print(f\"最终模型已保存到: {final_model_path}\")\n",
    "\n",
    "# 可视化3: 最终模型特征重要性\n",
    "plt.figure(figsize=(10, 8))\n",
    "xgb.plot_importance(final_model, max_num_features=20, height=0.8, title=\"Feature Importance (Final Model)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PLOT_PATH / \"final_model_feature_importances.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- 步骤 5: 在最终测试集上进行无偏评估 ---\n",
    "print(\"\\n--- [步骤 5] 在最终测试集上进行无偏评估 ---\")\n",
    "y_pred_proba_final = final_model.predict_proba(X_final_test)[:, 1]\n",
    "y_pred_final = (y_pred_proba_final >= 0.5).astype(int)\n",
    "\n",
    "print(\"--- 最终模型在最终测试集上的性能 ---\")\n",
    "print(f\"Accuracy : {accuracy_score(y_final_test, y_pred_final):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_final_test, y_pred_final):.4f}\")\n",
    "print(f\"Recall   : {recall_score(y_final_test, y_pred_final):.4f}\")\n",
    "print(f\"F1 Score : {f1_score(y_final_test, y_pred_final):.4f}\")\n",
    "print(f\"AUC      : {roc_auc_score(y_final_test, y_pred_proba_final):.4f}\")\n",
    "\n",
    "# 可视化4: 最终测试集上的混淆矩阵\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ConfusionMatrixDisplay.from_predictions(y_final_test, y_pred_final,\n",
    "                                        ax=ax,\n",
    "                                        display_labels=['Absence', 'Presence'],\n",
    "                                        cmap='Blues')\n",
    "ax.set_title('Confusion Matrix on Final Test Set')\n",
    "plt.savefig(OUTPUT_PLOT_PATH / \"final_model_confusion_matrix.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- 整体流程执行完毕 ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556c738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xgb_gpu_env)",
   "language": "python",
   "name": "xgb_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
